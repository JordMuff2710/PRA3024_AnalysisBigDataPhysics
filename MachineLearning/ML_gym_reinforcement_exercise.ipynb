{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iv2rJ1irpVc"
   },
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "\n",
    "# GYM provides an environment for an agent to interact with. We'll look at the 'cartpole',\n",
    "#  or the unstable, top-heavy inverted pendulum on top of a cart, which our 'agent' must learn to balance.\n",
    "#\n",
    "# GYM provides:\n",
    "#  - a state (observation) of the system/environment\n",
    "#    (in case of cartpole: cart position and velocity, angle and velocity at the tip\n",
    "#  - a set of _actions_ the agent can undertake in the environment\n",
    "#    (in case of cartpole: moving the cart base left or right)\n",
    "\n",
    "# When an agent performs a certain action in the current enviroment, GYM returns\n",
    "#  - a _reward_ for a certain result of performing the action in the environment\n",
    "#    (in case of cartpole: +1 if it didn't die.)\n",
    "#  - the new _state_ of the environment after performing said action\n",
    "#    (in case of cartpole: the effect of gravity pulling on the pendulum's top, on the angle, in one time step)\n",
    "#  - a 'terminal' in case the state is such that the agent cannot continue (is 'dead')\n",
    "#    (in case of cartpole: if the angle exceeds > 5 degrees of tipping over, or it flies off-screen)\n",
    "\n",
    "# It is up to us to make an agent that learns which action to take in which state, to maximise the total reward before it dies.\n",
    "#  We do this by 'remembering' what the results were of certain actions on certain states in a memory (i.e. a list or deque),\n",
    "#  And after every 'death' we sample ('replay') from this memory to train a neural network that decides the best actions\n",
    "#   we can take on any state we've encountered so far, which maximises the total reward.\n",
    "\n",
    "# In addition, GYM provides a nice real-time visual output of the environment and the result of actions taken.\n",
    "#  In google COLAB however, we pull some strings, and only have a combined video at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVf34L_Q0rE_"
   },
   "outputs": [],
   "source": [
    "COLAB = True\n",
    "\n",
    "if(COLAB) :\n",
    "  # install some helpers to visualize gym graphics in the colab environment\n",
    "  !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "  !apt-get install -y xvfb python-opengl ffmpeg x11-utils > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "fA7nN0_rrwRa",
    "outputId": "c36f9fe2-413e-4f25-e087-a12a0b6f28a6"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRnwGNBeuNbY"
   },
   "outputs": [],
   "source": [
    "# only necessary for visualisation in colab env.\n",
    "if(COLAB) :\n",
    "  from IPython.display import HTML\n",
    "  from IPython import display as ipythondisplay\n",
    "  from pyvirtualdisplay import Display\n",
    "  from gym.wrappers import Monitor\n",
    "  import glob, io, os, base64\n",
    "  from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "  display = Display(visible=0, size=(1400, 900))\n",
    "  display.start()\n",
    "  os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "\n",
    "  def combine_videos() :\n",
    "    mp4list = sorted( glob.glob('video/*.mp4') )\n",
    "    if len(mp4list) > 0 :\n",
    "      videolist = []\n",
    "      for mp4 in mp4list :\n",
    "        videolist.append( VideoFileClip(mp4))\n",
    "      final_clip = concatenate_videoclips(videolist)\n",
    "      final_clip.to_videofile(\"combined.mp4\", fps=24, remove_temp=False)\n",
    "      return 1\n",
    "    else :\n",
    "      return 0\n",
    "\n",
    "  def show_video() :\n",
    "      if(combine_videos()) :\n",
    "        video = io.open(\"combined.mp4\", 'r+b').read()\n",
    "\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                  loop controls style=\"height: 400px;\">\n",
    "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "               </video>'''.format(encoded.decode('ascii'))))\n",
    "      else :\n",
    "        print(\"Error: no videos found\")     \n",
    "\n",
    "\n",
    "  def wrap_env(env):\n",
    "    env = Monitor(env, './video', video_callable=lambda episode_id: True, force=True)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iVzZ7nmErzCw"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "### STUDENT CODE HERE ###\n",
    "### --> Change the parameters below to perform a better training\n",
    "\n",
    "\n",
    "MAXRUNS = 50\n",
    "#NSTEPSOLVED = 1000\n",
    "\n",
    "GAMMA = 0.5  # 'future discount factor'\n",
    "LEARNING_RATE = 0.000001 # rate at which to update weights after each training step\n",
    "\n",
    "MEMORY_SIZE = 100 # size of container to hold actions and outcomes\n",
    "BATCH_SIZE = 5 # number of actions in memory to 'replay' after each death\n",
    "\n",
    "# probability to do 'random' actions, to sample from event space\n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2bZXXrqr5Tj"
   },
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        # This will be a simple feed-forward NN, with \n",
    "        #  - input = 'observation' (aka state)\n",
    "        #  - output = predicted 'quality' of each possible action\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        ### STUDENT CODE HERE ###\n",
    "        ### --> Write the network using fully-connected (Dense) layers.\n",
    "        ###  Make sure the input has the same shape as an observation/state,\n",
    "        ###   and the output has the same dimensions as the number of possible actions.\n",
    "        ### END STUDENT CODE ###\n",
    "        \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "        self.model.summary()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # add event to memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # return the best possible action for the current state\n",
    "\n",
    "        # sometimes allow for a random action at the 'exploration rate', to avoid local minima\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        \n",
    "        # Get predicted qualities for each possible action, and return the action (=index) with the highest quality\n",
    "        q_values = self.model.predict(state) \n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        # Learn from random subset of memory (reduces corr. between subsequent actions).\n",
    "        # learning is done by comparing 'predicted quality' to the here defined quality (~reward) of the action.\n",
    "        \n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            # We haven't experienced enough to properly learn yet - keep exploring!\n",
    "            return\n",
    "        \n",
    "        # Get random subset of memory\n",
    "        batch = random.sample(self.memory, BATCH_SIZE) \n",
    "        \n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "\n",
    "            # We define the 'quality' of a move by taking the known, memorized reward for the action,\n",
    "            #  and adding the predicted quality of the (predicted) best choice of action for the next state, to that.\n",
    "            # As the model learns to give this situation a low quality, any step leading up to this state will get a \n",
    "            #  lower quality due to the predict(state_next) term. This will slowly trickle through to the step before that, etc.,\n",
    "            #  slowly making our agent learn about future consequences of current actions.\n",
    "          \n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                \n",
    "                # One could try to make the model learn from intermediate steps directly as well, speeding up the learning,\n",
    "                #  e.g. by changing the reward based on an increase in angle (although this should really be defined in the env.)\n",
    "                #q_update -= 1.0 * abs(state_next[0][2]) - abs(state[0][2]) # penalize angle increases for cartpole\n",
    "                \n",
    "            # - Define the quality of the non-chosen action to just be the predicted quality (i.e. diff = 0)\n",
    "            # - Define the quality of the chosen action to be the newly defined quality\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update \n",
    "            \n",
    "            # Finally, find the optimal model weights for minimal difference between \n",
    "            #  predicted quality and observed quality (+ future prediction as per above) for this action.\n",
    "            # The weights are then updated * learning rate\n",
    "            self.model.fit(state, q_values, verbose=0) \n",
    "            \n",
    "        # reduce the 'random choices' rate over time, because you expect the model to have learned\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCAlTnFAr8M8"
   },
   "outputs": [],
   "source": [
    "def cartpole() :\n",
    "    if(COLAB) : env = wrap_env(gym.make(ENV_NAME)) # only for visualisation in colab\n",
    "    else : env = gym.make(ENV_NAME)\n",
    "\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "       \n",
    "    run=0\n",
    "    runsteplog = []\n",
    "    #while True:\n",
    "    for i in range(MAXRUNS):\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        \n",
    "        while True :    \n",
    "            step += 1\n",
    "            screen = env.render() # graphical output\n",
    "\n",
    "\n",
    "            \n",
    "            ### STUDENT CODE HERE\n",
    "            ### --> Redefine the action to be the output of the 'act' of your solver.\n",
    "            action = env.action_space.sample() # (this takes a random action)\n",
    "            ### END STUDENT CODE\n",
    "            \n",
    "            # make the action\n",
    "            state_next, reward, terminal, info = env.step(action) \n",
    "\n",
    "            # if action made terminal: reduce reward!\n",
    "            reward = reward if not terminal else -reward  \n",
    "\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "\n",
    "            # fill agent memory with this action's results\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal) \n",
    "\n",
    "            # prepare for the next action in the environment\n",
    "            state = state_next \n",
    "            \n",
    "            if terminal :\n",
    "                #state = env.reset()\n",
    "                #state = np.reshape(state, [1, observation_space])\n",
    "                print(\"Run: {0}, exploration: {1:.15f}, score: {2}\".format(run,dqn_solver.exploration_rate,step))\n",
    "                runsteplog += [step]\n",
    "\n",
    "                break\n",
    "            \n",
    "            dqn_solver.experience_replay() # learn from batch of memories every time a new one is made\n",
    "            \n",
    "            #if(step > NSTEPSOLVED) :\n",
    "            #    i = MAXRUNS\n",
    "            #    print(\"Solved! (step > NSTEPSOLVED)\")\n",
    "            #    break\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "\n",
    "\n",
    "    # Show training process\n",
    "    \n",
    "    # plot #steps achieved\n",
    "    plt.plot(runsteplog)\n",
    "    plt.ylabel(\"# actions before terminal\")\n",
    "    plt.xlabel(\"run iteration\")\n",
    "\n",
    "    success_measure = np.mean(runsteplog[-15:])\n",
    "    print(\"Mean of last 15 runs: {0}\".format(success_measure))\n",
    "    return success_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RBDY7DTDr-uK",
    "outputId": "50dea2ff-c532-4a5c-9768-7735324fb05f"
   },
   "outputs": [],
   "source": [
    "MAXRUNS = 50\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cartpole()\n",
    "\n",
    "if(COLAB) :\n",
    "    show_video() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecN2oFiWsAkH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_gym_reinforcement.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
